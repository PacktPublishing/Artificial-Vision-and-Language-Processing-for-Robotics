{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Exercise01.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "mOUh5tTXroVe",
        "colab_type": "code",
        "outputId": "edf42813-c84c-4b40-da44-b77e5e0c26c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize \n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "wZJqiY1JsEzg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G3n51hLCrxeo",
        "colab_type": "code",
        "outputId": "a6dd521b-48a7-4b84-d358-f927873567fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "cell_type": "code",
      "source": [
        "example_sentence = \"This course is great. I'm going to learn deep learning; Artificial Intelligence is amazing and I love robotics...\"\n",
        "sent_tokenize(example_sentence) # Divide the text into sentences\n",
        "word_tokenize(example_sentence) # Divide the text into words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'course',\n",
              " 'is',\n",
              " 'great',\n",
              " '.',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'going',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'deep',\n",
              " 'learning',\n",
              " ';',\n",
              " 'Artificial',\n",
              " 'Intelligence',\n",
              " 'is',\n",
              " 'amazing',\n",
              " 'and',\n",
              " 'I',\n",
              " 'love',\n",
              " 'robotics',\n",
              " '...']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "KxrUjy3JsF-I",
        "colab_type": "code",
        "outputId": "5076769e-2eb9-4e7f-e209-2949dd6ca0b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "SYif5GjjsJsg",
        "colab_type": "code",
        "outputId": "4904aa99-8f20-41b5-dc56-fa39dbf36889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\")) \n",
        "print(stop_words)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'mightn', 'than', 've', 'during', 'whom', 'had', 'on', 'll', 'been', \"doesn't\", \"she's\", 'they', 'y', 'd', 'weren', \"it's\", \"that'll\", \"should've\", 'just', 's', 'very', 'in', 'those', \"hadn't\", 'yours', 'haven', 'its', 'herself', 'wasn', 'down', 'my', 'what', 'until', 'don', 'does', 'or', 'own', 'didn', 'into', 'should', 'are', 'she', 'if', 'too', 'yourself', 'did', 'here', 'only', 'you', 'again', \"mightn't\", 'nor', 'being', 'over', 'couldn', 'is', 'ma', 'and', \"don't\", 'the', \"shan't\", 'each', \"needn't\", 'while', 'yourselves', 'an', 'then', \"isn't\", 'it', 'up', 'me', \"won't\", 'ourselves', 'such', 'hasn', 'further', 'needn', \"weren't\", 'below', 'to', 'but', 'won', 'as', 'there', 'under', 'when', 'no', 'shan', 'ain', 'that', \"shouldn't\", 'most', 'for', 'not', 'her', 'by', \"couldn't\", 'be', 'shouldn', \"you'd\", 'mustn', 'we', 'off', 'doing', 'so', 'our', 'why', 'some', 'm', \"didn't\", \"wouldn't\", 'out', 'itself', 'will', \"you're\", 'o', 'he', 'hers', 'myself', 'through', 'himself', 'above', 'more', 'both', \"you've\", 'were', 'his', 'your', \"haven't\", 'am', 'with', 'because', \"wasn't\", 'from', 'after', 'how', 'ours', 'of', 'doesn', 'before', \"hasn't\", 'can', 'him', 'themselves', 'now', 'once', 'these', 'where', 'few', 'other', 'theirs', 'isn', \"you'll\", 'i', 'any', 'wouldn', \"aren't\", 'their', 'having', 'which', 'a', 'who', 'do', 'them', 'all', 'aren', 'between', 'about', 'have', 'was', 're', 'hadn', 'same', 'at', 'has', \"mustn't\", 't', 'this', 'against'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wBVMF_FvsMhx",
        "colab_type": "code",
        "outputId": "3ce9cf42-ec2c-48e8-cf74-91b0dd13f294",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print([w for w in word_tokenize(example_sentence.lower()) if w not in stop_words]) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['course', 'great', '.', \"'m\", 'going', 'learn', 'deep', 'learning', ';', 'artificial', 'intelligence', 'amazing', 'love', 'robotics', '...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "az4ORMFbsPQ4",
        "colab_type": "code",
        "outputId": "a850c4f5-a8cb-4446-92dc-62bbfef0f759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "stop_words = stop_words - set(('this', 'i', 'and')) \n",
        "print([w for w in word_tokenize(example_sentence.lower()) if w not in stop_words]) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['this', 'course', 'great', '.', 'i', \"'m\", 'going', 'learn', 'deep', 'learning', ';', 'artificial', 'intelligence', 'amazing', 'and', 'i', 'love', 'robotics', '...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X9lrMA2csR2g",
        "colab_type": "code",
        "outputId": "b333a905-6001-4a53-d18e-7b1c9d577118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer() \n",
        "print([stemmer.stem(w) for w in  word_tokenize(example_sentence)])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['thi', 'cours', 'is', 'great', '.', 'I', \"'m\", 'go', 'to', 'learn', 'deep', 'learn', ';', 'artifici', 'intellig', 'is', 'amaz', 'and', 'I', 'love', 'robot', '...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1_J_vEiusUSA",
        "colab_type": "code",
        "outputId": "67d1b78d-9fb2-4ee6-d12b-3fc49d3ff508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "t = nltk.pos_tag(word_tokenize(example_sentence)) #words with each tag\n",
        "t"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('This', 'DT'),\n",
              " ('course', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('great', 'JJ'),\n",
              " ('.', '.'),\n",
              " ('I', 'PRP'),\n",
              " (\"'m\", 'VBP'),\n",
              " ('going', 'VBG'),\n",
              " ('to', 'TO'),\n",
              " ('learn', 'VB'),\n",
              " ('deep', 'JJ'),\n",
              " ('learning', 'NN'),\n",
              " (';', ':'),\n",
              " ('Artificial', 'NNP'),\n",
              " ('Intelligence', 'NNP'),\n",
              " ('is', 'VBZ'),\n",
              " ('amazing', 'JJ'),\n",
              " ('and', 'CC'),\n",
              " ('I', 'PRP'),\n",
              " ('love', 'VBP'),\n",
              " ('robotics', 'NNS'),\n",
              " ('...', ':')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}